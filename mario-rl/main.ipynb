{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is made for Reasoning Agents part where the fundamental idea is to apply Reinforcement Learning to Mario game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "We have included several libraries which will be needed for the project. They are Gym, pandas, numpy, tensorflow and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import gym\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from keras.models import save_model, load_model\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, RIGHT_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Simple test\n",
    "This is used to test the game and see how it exactly works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# total_reward = 0\n",
    "# done = True\n",
    "# for step in range(100000):\n",
    "#     env.render()\n",
    "#     if done:\n",
    "#         state = env.reset()\n",
    "#     state, reward, done, info = env.step(env.action_space.sample())\n",
    "#     preprocess_state(state)\n",
    "#     print(info)\n",
    "#     total_reward += reward\n",
    "#     clear_output(wait=True)\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class for the Mario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "#preprocess_state(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MarioAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # we need to create variables for our agent here\n",
    "        self.state_space = state_size\n",
    "        self.action_space = action_size\n",
    "        self.memory = deque(maxlen=5000)\n",
    "        self.gamma = 0.8\n",
    "        self.chosenAction = 0\n",
    "        #giving the epsilon value for exploration and exploiation\n",
    "        self.epsilon = 1\n",
    "        self.max_epsilon = 1\n",
    "        self.min_epsilon = 0.01\n",
    "        self.decay_epsilon = 0.0001\n",
    "\n",
    "        # we then need to build the NN for the agent\n",
    "        self.main_network = self.build_network()\n",
    "        self.target_network = self.build_network()\n",
    "        self.update_target_network()\n",
    "\n",
    "    def build_network(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(64, (4,4), strides=4, padding='same',input_shape=self.state_space))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        model.add(Conv2D(64, (4,4), strides=2, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        model.add(Conv2D(64, (3,3), strides=1, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Flatten())\n",
    "\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(self.action_space, activation='linear'))\n",
    "\n",
    "        model.compile(loss='mse', optimizer=Adam())\n",
    "        return model\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.set_weights(self.main_network.get_weights())\n",
    "\n",
    "    def act(self, state, onGround):\n",
    "        if onGround < 83:\n",
    "            print('Mario is on Ground')\n",
    "            if random.uniform(0,1) < self.epsilon:\n",
    "                self.chosenAction = np.random.randint(self.action_space)\n",
    "                return self.chosenAction\n",
    "            Q_value = self.main_network.predict(state)\n",
    "            self.chosenAction = np.argmax(Q_value[0])\n",
    "            return self.chosenAction\n",
    "        else:\n",
    "            print('Mario is not on Ground')\n",
    "            return self.chosenAction\n",
    "\n",
    "\n",
    "    def update_epsilon(self, episode):\n",
    "        self.epsilon = self.min_epsilon + (self.max_epsilon - self.min_epsilon)*np.exp(-self.decay_epsilon * episode)\n",
    "\n",
    "    def train(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.main_network.predict(state)\n",
    "\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                target[0][action] = (reward+ self.gamma * np.amax(self.target_network.predict(next_state)))\n",
    "\n",
    "            self.main_network.fit(state, target, epochs=1, verbose=0)\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def get_pred_act(self, state):\n",
    "        Q_values = self.main_network.predict(state)\n",
    "        return np.argmax(Q_values[0])\n",
    "    def load(self, name):\n",
    "        self.main_network = load_model(name)\n",
    "        self.target_network = load_model(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        save_model(self.main_network, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "action_space = env.action_space.n\n",
    "state_space = (80,88,1)\n",
    "env.observation_space\n",
    "from PIL import Image\n",
    "\n",
    "def preprocess_state(state):\n",
    "    image = Image.fromarray(state)\n",
    "    image = image.resize((88,80))\n",
    "    image = image.convert('L')\n",
    "    image = np.array(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_episodes = 1000000\n",
    "num_timesteps = 400000\n",
    "batch_size = 64\n",
    "DEBUG_LEN = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mario = MarioAgent(state_space, action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stuck_buffer = deque(maxlen=DEBUG_LEN)\n",
    "for i in range(num_episodes):\n",
    "    Return = 0\n",
    "    done = False\n",
    "    time_step = 0\n",
    "    onGround = 79\n",
    "\n",
    "    state = preprocess_state(env.reset())\n",
    "    state = state.reshape(-1, 80, 88, 1)\n",
    "\n",
    "    for t in range(num_timesteps):\n",
    "        env.render()\n",
    "        time_step += 1\n",
    "\n",
    "        if t>1 and stuck_buffer.count(stuck_buffer[-1]) == DEBUG_LEN-50:\n",
    "            action = mario.act(state, onGround=79)\n",
    "        else:\n",
    "            action = mario.act(state, onGround)\n",
    "\n",
    "        action = mario.act(state, onGround)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        onGround = info['y_pos']\n",
    "        stuck_buffer.append(info['x_pos'])\n",
    "\n",
    "        next_state = preprocess_state(next_state)\n",
    "        next_state = next_state.reshape(-1, 80,88,1)\n",
    "\n",
    "        mario.store_transition(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        Return += reward\n",
    "        print(\"Episode is: {}\\nTotal Time Step: {}\\nCurrent Reward: {}\\nEpsilon is: {}\".format(str(i), str(time_step), str(Return), str(mario.epsilon)))\n",
    "\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        if len(mario.memory) > batch_size and i > 5:\n",
    "            mario.train(batch_size)\n",
    "    mario.update_epsilon(i)\n",
    "    clear_output(wait=True)\n",
    "    mario.update_target_network()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mario.save('mario-v0.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mario.load('mario-v0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while 1:\n",
    "    done = False\n",
    "    state = preprocess_state(env.reset())\n",
    "    state = state.reshape(-1, 80,88,1)\n",
    "    total_reward = 0\n",
    "    onGround = 79\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = mario.act(state, onGround)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        onGround = info['y_pos']\n",
    "        next_state = preprocess_state(next_state)\n",
    "        next_state = next_state.reshape(-1, 80, 88, 1)\n",
    "        state = next_state\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
